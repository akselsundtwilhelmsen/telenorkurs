{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and activate a virtual environment (Optional)\n",
    "- `python -m venv openai-env`\n",
    "- `source openai-env/bin/activate` (Mac)\n",
    "- `openai-env\\Scripts\\activate` (Windows)\n",
    "\n",
    "Once the virtial environment is set up, install the OpenAI Python library:\n",
    "- `pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code is setting up three string constants:\n",
    "\n",
    "- OPENAI_API_KEY: This is the API key for OpenAI. It's used to authenticate your application when making requests to the OpenAI API.\n",
    "\n",
    "- EMBEDDING_MODEL: This is the name of the model used for text embedding. Text embedding is a way to convert text into a form that can be processed by machine learning algorithms. In this case, the model is \"text-embedding-ada-002\".\n",
    "\n",
    "- LLM: This is the name of the language model used by OpenAI. We recommend using `gpt-3.5-turbo-0125` due to superior latency. `gpt-4-0125-preview`is, however, the most capable model with a 128k token context window and have been trained on data up to December 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "LLM = \"gpt-3.5-turbo-0125\" #'gpt-4-0125-preview' better with large inputs and more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the available models you can choose to experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-3.5-turbo-16k',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'dall-e-3',\n",
       " 'gpt-3.5-turbo-16k-0613',\n",
       " 'dall-e-2',\n",
       " 'text-embedding-3-large',\n",
       " 'whisper-1',\n",
       " 'tts-1-hd-1106',\n",
       " 'tts-1-hd',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'gpt-3.5-turbo-0613',\n",
       " 'gpt-3.5-turbo-instruct-0914',\n",
       " 'gpt-4',\n",
       " 'tts-1',\n",
       " 'davinci-002',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'babbage-002',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-vision-preview',\n",
       " 'tts-1-1106',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-turbo-preview',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-embedding-3-small',\n",
       " 'davinci:ft-anderssl-2022-06-29-11-12-44',\n",
       " 'davinci:ft-anderssl-2022-06-29-14-39-18']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [model.id for model in client.models.list().data]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code uses the OpenAI API to create a chat completion. It's using the GPT-4 model to generate responses to a user's question about Telenor.\n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "- `client.chat.completions.create`: This is a method from the OpenAI API that creates a chat completion. A chat completion is a conversation with the model where you provide a series of messages and the model returns a generated message as a response.\n",
    "\n",
    "- `model=\"gpt-4-0125-preview\"`: This specifies the model to use for the chat completion. In this case, it's using the GPT-4 model.\n",
    "\n",
    "- `messages`: This is a list of messages to send to the model. Each message is a dictionary with two keys: 'role' and 'content'. 'role' can be 'system', 'user', or 'assistant', and 'content' is the text of the message. The 'system' role is used to set the behavior of the 'assistant', and the 'user' role is used to ask the assistant a question.\n",
    "\n",
    "In this example, the system message sets the assistant's role as a helpful assistant that can answer questions about Telenor. The user message then asks the question \"What does Telenor do?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Telenor er et telekommunikasjonsselskap som tilbyr tjenester innen mobiltelefoni, bredbånd og TV. Selskapet driver også med nettverksinfrastruktur, satellittkommunikasjon og digitale tjenester. Telenor har virksomhet i flere land, hovedsakelig i Skandinavia og Asia.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=LLM,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Du er en hjelpsom assistent som kan svare på spørsmål om Telenor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hva driver Telenor med?\"}\n",
    "        ]\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code is using the OpenAI API to create an embedding for a given text input.\n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "- `client.embeddings.create()`: This is a method from the OpenAI API that creates an embedding. An embedding is a vector representation of the input text. It's a way of converting text into a form that can be processed by machine learning algorithms. It also enables you to perform vector search using e.g. cosine similarity. \n",
    "\n",
    "- `model=\"text-embedding-ada-002\"`: This specifies the model to use for creating the embedding. In this case, it's using the \"text-embedding-ada-002\" model.\n",
    "\n",
    "- `input=\"Verdifull informasjon om Telenor som du vil bruke inn i språkmodellen.\"`: This is the text input for which the embedding will be created.\n",
    "\n",
    "- `encoding_format=\"float\"`: This specifies the format of the encoding for the embedding. In this case, it's set to \"float\", which means the embedding will be a list of floating-point numbers.\n",
    "\n",
    "In this example, the code is creating an embedding for the text \"Verdifull informasjon om Telenor som du vil bruke inn i språkmodellen.\" using the \"text-embedding-ada-002\" model and a floating-point encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model_name: str):\n",
    "    embedding = client.embeddings.create(\n",
    "        model=model_name,\n",
    "        input=text,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "example_embedding = get_embedding(\"Verdifull informasjon om Telenor som du vil bruke inn i språkmodellen.\", EMBEDDING_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `cos_sim` is calculating the cosine similarity between two vectors `a` and `b`. If the vectors are identical, the cosine similarity is 1. If the vectors are orthogonal (i.e., not similar at all), the cosine similarity is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API has a limit on the maximum number of input tokens for embeddings. The following function calculates number of tokens from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced set-ups of applications powered by LLMs, check out these links:\n",
    "\n",
    "- https://python.langchain.com/docs/get_started/introduction\n",
    "- https://microsoft.github.io/autogen/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
